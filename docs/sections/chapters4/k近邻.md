# K-近邻算法

## 一、简介

电影可以按照题材分类，每个题材又是如何定义的呢？那么假如两种类型的电影，动作片和爱情片。动作片有哪些公共的特征？那么爱情片又存在哪些明显的差别呢？我们发现动作片中打斗镜头的次数较多，而爱情片中接吻镜头相对更多。当然动作片中也有一些接吻镜头，爱情片中也会有一些打斗镜头。所以不能单纯通过是否存在打斗镜头或者接吻镜头来判断影片的类别。那么现在我们有6部影片已经明确了类别，也有打斗镜头和接吻镜头的次数，还有一部电影类型未知。

|         电影名称          | 打斗镜头 | 接吻镜头 | 电影类型 |
| :-----------------------: | :------: | :------: | :------: |
|      California Man       |    3     |   104    |  爱情片  |
| He's not Really into dues |    2     |   100    |  爱情片  |
|      Beautiful Woman      |    1     |    81    |  爱情片  |
|      Kevin Longblade      |   101    |    10    |  动作片  |
|     Robo Slayer 3000      |    99    |    5     |  动作片  |
|         Amped II          |    98    |    2     |  动作片  |
|             ?             |    18    |    90    |   未知   |

那么我们使用K-近邻算法来分类爱情片和动作片：存在一个样本数据集合，也叫训练样本集，样本个数M个，知道每一个数据特征与类别对应关系，然后存在未知类型数据集合1个，那么我们要选择一个测试样本数据中与训练样本中M个的距离，排序过后选出最近的K个，这个取值一般不大于20个。选择K个最相近数据中次数最多的分类。那么我们根据这个原则去判断未知电影的分类

|         电影名称          | 与未知电影的距离 |
| :-----------------------: | :--------------: |
|      California Man       |       20.5       |
| He's not Really into dues |       18.7       |
|      Beautiful Woman      |       19.2       |
|      Kevin Longblade      |      115.3       |
|     Robo Slayer 3000      |      117.4       |
|         Amped II          |      118.9       |

我们假设K为3，那么排名前三个电影的类型都是爱情片，所以我们判定这个未知电影也是一个爱情片。

K-近邻算法（K-Nearest Neighbors，简称KNN）是一种常用的分类和回归算法。它基于实例之间的相似性进行预测，并且不需要显式的训练过程。KNN算法的核心思想是：如果一个样本的K个最近邻居中的大多数属于某个类别，那么该样本很有可能属于该类别。

## 二、KNN算法步骤

KNN算法的基本步骤如下：

1. 准备数据集：收集带有标签的训练样本，其中包括特征和对应的类别标签。
2. 计算距离：对于测试样本，计算它与每个训练样本之间的距离。常用的距离度量方法包括欧氏距离、曼哈顿距离等。
3. 选择K值：确定K的取值，即要考虑的最近邻居的数量。
4. 选择最近邻：选择与测试样本最近的K个训练样本。
5. 进行投票：根据最近邻居的类别标签进行投票，将得票最多的类别作为测试样本的预测类别。
6. 输出预测结果：将预测类别作为测试样本的分类结果。

## 三、距离公式

KNN算法中最关键的部分之一是计算样本之间的距离，以找到最近的邻居。通常情况下，KNN算法使用欧氏距离（Euclidean Distance）作为默认的距离度量，但也可以选择其他距离度量方法，如曼哈顿距离（Manhattan Distance）等。

以下是KNN算法中常用的距离度量公式：

### 1. 欧氏距离（Euclidean Distance）

对于两个样本向量x和y，欧氏距离可以通过以下公式计算： 
$$
\sum_{i=1}^n | x_i - y_i |
$$
这里n表示向量的维度，xi和yi分别表示向量x和y在第i个维度上的取值。

举例： 假设我们有两个二维样本向量x=[1, 2]和y=[4, 6]，我们可以使用欧氏距离公式计算它们之间的距离： 
$$
\sqrt{(1-4)^2 + (2-6)^2} = \sqrt{9+16} = 5
$$

### 2. 曼哈顿距离（Manhattan Distance） 

在曼哈顿街区要从一个十字路口开车到另一个十字路口，驾驶距离显然不是两点间的直线距离。这个实际驾驶距离就是“曼哈顿距离”。曼哈顿距离也称为“城市街区距离”(City Block distance)。

![img](https://raw.githubusercontent.com/sanmaomashi/Salute_Machine_Learning/main/img/3.png)

曼哈顿距离是通过计算两个样本向量中对应维度数值差的绝对值之和来度量距离的。 对于两个样本向量x和y，曼哈顿距离可以通过以下公式计算：
$$
\sum_{i=1}^n | x_i - y_i |
$$
举例： 假设我们有两个二维样本向量x=[1, 2]和y=[4, 6]，我们可以使用曼哈顿距离公式计算它们之间的距离： 
$$
| 1-4 | + | 2-6 | = 3 + 4 = 7
$$
### 3. 切比雪夫距离 (Chebyshev Distance)

国际象棋中，国王可以直行、横行、斜行，所以国王走一步可以移动到相邻8个方格中的任意一个。国王从格子(x1,y1)走到格子(x2,y2)最少需要多少步？这个距离就叫切比雪夫距离。

![img](https://raw.githubusercontent.com/sanmaomashi/Salute_Machine_Learning/main/img/4.png)

切比雪夫距离（Chebyshev Distance）是一种衡量两个向量之间的距离的度量方式。它表示两个向量在每个维度上的最大差值，即在所有维度上的差值中取最大值。切比雪夫距离的公式如下所示：
$$
d_{\text{Chebyshev}}(\mathbf{X}, \mathbf{Y}) = \max_i |x_i - y_i|
$$
其中，`d`表示向量`X`和`Y`之间的切比雪夫距离，`xᵢ`和`yᵢ`分别表示向量`X`和`Y`在第`i`个维度上的值，`|xᵢ - yᵢ|`表示差值的绝对值，而`maxᵢ`表示在所有维度上取最大值。

切比雪夫距离适用于衡量两个向量之间的最大差异，例如在图像处理中用于测量两个图像的差异，或者在多维数据集中用于聚类分析等。

举例： 假设我们有两个二维样本向量x=[1, 2]和y=[4, 6]，我们可以使用切比雪夫距离公式来计算它们之间的距离。

将向量 `x` 和 `y` 的值代入公式中，我们可以计算切比雪夫距离如下：
$$
d_{\text{Chebyshev}}(\mathbf{X}, \mathbf{Y}) = \max(|1 - 4|, |2 - 6|)
                                            = \max(3, 4)
                                            = 4
$$
因此，向量 `x = [1, 2]` 和 `y = [4, 6]` 之间的切比雪夫距离为 4。

需要注意的是，切比雪夫距离对异常值比较敏感，因为它只考虑每个维度上的最大差异，而不考虑其他维度上的差异。因此，在使用切比雪夫距离时，需要注意数据的特性以及是否适用于具体的应用场景。

### 4. 闵可夫斯基距离(Minkowski Distance)

闵可夫斯基距离（Minkowski Distance）是一种常用的距离度量方法，它是欧几里德距离和曼哈顿距离的一般化形式。它表示两个向量之间的距离，并且可以调整参数 p 来控制距离的度量方式。

闵可夫斯基距离的公式如下所示：
$$
d_{\text{Minkowski}}(\mathbf{X}, \mathbf{Y}) = \left( \sum_{i=1}^n |x_i - y_i|^p \right)^{\frac{1}{p}}
$$
其中，`d` 表示向量 `X` 和 `Y` 之间的闵可夫斯基距离，`xᵢ` 和 `yᵢ` 分别表示向量 `X` 和 `Y` 在第 `i` 个维度上的值，`|xᵢ - yᵢ|` 表示差值的绝对值，`n` 表示向量的维度，`p` 是控制度量方式的参数。

根据参数 `p` 的不同取值，闵可夫斯基距离可以衍生出不同的距离度量方式。特殊情况下，当 `p=1` 时，闵可夫斯基距离等同于曼哈顿距离；当 `p=2` 时，闵可夫斯基距离等同于欧几里德距离。

下面给出一个计算闵可夫斯基距离的例子：

假设有两个向量 `x = [1, 2, 3]` 和 `y = [4, 6, 8]`，我们计算它们之间的闵可夫斯基距离，其中参数 `p=3`：
$$
d_{\text{Minkowski}}(\mathbf{X}, \mathbf{Y}) = \left( |1-4|^3 + |2-6|^3 + |3-8|^3 \right)^{\frac{1}{3}}
                                            = \left( |-3|^3 + |-4|^3 + |-5|^3 \right)^{\frac{1}{3}}
                                            = (27 + 64 + 125)^{\frac{1}{3}}
                                            = 216^{\frac{1}{3}}
                                            = 6
$$
因此，向量 `x = [1, 2, 3]` 和 `y = [4, 6, 8]` 之间的闵可夫斯基距离（当参数 `p=3`）为 6。

通过调整参数 `p` 的值，可以改变闵可夫斯基距离的度量方式，使其适应不同的数据和应用场景。当 `p=1` 时，闵可夫斯基距离等同于曼哈顿距离；当 `p=2` 时，闵可夫斯基距离等同于欧几里德距离。

### 5. 标准化欧氏距离 (Standardized EuclideanDistance)

标准化欧氏距离（Standardized Euclidean Distance）是欧氏距离的一种变体，用于在比较具有不同尺度或变化范围的特征时进行距离度量。它在计算距离之前对特征进行标准化，以消除尺度差异的影响。

标准化欧氏距离的计算步骤如下：

1. 对特征进行标准化：对每个特征进行标准化处理，使其均值为0，标准差为1。这可以通过减去均值并除以标准差来实现。
2. 计算标准化欧氏距离：对于两个向量 `x` 和 `y`，计算它们之间的标准化欧氏距离，使用以下公式：

$$
d_{\text{StdEuclidean}}(\mathbf{X}, \mathbf{Y}) = \sqrt{\sum_{i=1}^n (x_i - y_i)^2}
$$

其中，`d` 表示向量 `X` 和 `Y` 之间的标准化欧氏距离，`xᵢ` 和 `yᵢ` 分别表示向量 `X` 和 `Y` 在第 `i` 个特征上的标准化值，`n` 表示特征的数量。

标准化欧氏距离的优点是可以消除特征之间尺度差异的影响，使得距离度量更加公平和准确。它适用于处理具有不同尺度特征的数据，例如在聚类分析、异常检测或特征选择等任务中。

需要注意的是，在计算标准化欧氏距离之前，特征的标准化处理是必要的。这可以通过使用各种标准化方法，如Z-score标准化或最小-最大标准化等来实现。

### 6. 余弦距离(Cosine Distance)

余弦距离（Cosine Distance）是一种用于衡量两个向量之间的相似性的度量方法。它基于两个向量之间的夹角来计算相似度，而不考虑向量的绝对大小。余弦距离越小，表示两个向量越相似。

给定两个向量A和B，它们的余弦距离可以通过以下公式计算：
$$
\text{Cosine Distance}(A, B) = 1 - \frac{A \cdot B}{\|A\| \cdot \|B\|}
$$
其中，A·B表示向量A和向量B的内积（点积），|A|和|B|分别表示向量A和向量B的范数（长度）。

下面是一个示例：

假设有两个向量A和B，分别为：
$$
A = [1, 2, 3] 
B = [4, 5, 6]
$$
首先，计算向量A和向量B的内积：
$$
A · B = 1 * 4 + 2 * 5 + 3 * 6 = 32
$$
然后，计算向量A和向量B的范数：
$$
|A| = \sqrt{1^2 + 2^2 + 3^2} = \sqrt{14} ≈ 3.74 |B| = \sqrt{4^2 + 5^2 + 6^2} = \sqrt{77} ≈ 8.77
$$
最后，根据公式计算余弦距离：
$$
\text{{Cosine Distance}}(A, B) = 1 - \frac{{A \cdot B}}{{|A| \cdot |B|}} = 1 - \frac{{32}}{{3.74 \cdot 8.77}} ≈ 0.033
$$
因此，向量A和向量B的余弦距离约为0.033，表示它们之间非常相似。

注意：在某些情况下，对特征进行标准化或归一化可能对KNN算法的性能有所帮助，因为距离度量对特征的尺度敏感。

## 四、k值的选择

K值过小，容易受到异常点的影响；k值过大，容易受到样本均衡的问题

**K值选择问题，李航博士的一书「统计学习方法」上所说：**

1) 选择较小的K值，就相当于用较小的领域中的训练实例进行预测，“学习”近似误差会减小，只有与输入实例较近或相似的训练实例才会对预测结果起作用，与此同时带来的问题是“学习”的估计误差会增大，换句话说，**K值的减小就意味着整体模型变得复杂，容易发生过拟合；**
2) 选择较大的K值，就相当于用较大领域中的训练实例进行预测，其优点是可以减少学习的估计误差，但缺点是学习的近似误差会增大。这时候，**与输入实例较远（不相似的）训练实例也会对预测器作用，使预测发生错误，且K值的增大就意味着整体的模型变得简单。**
3) K=N（N为训练样本个数），则完全不足取，因为此时无论输入实例是什么，都只是简单的预测它属于在训练实例中最多的类，模型过于简单，忽略了训练实例中大量有用信息。

在实际应用中，K值一般取一个比较小的数值，例如采用交叉验证法（简单来说，就是把训练数据在分成两组:训练集和验证集）来选择最优的K值。对这个简单的分类器进行泛化，用核方法把这个线性模型扩展到非线性的情况，具体方法是把低维数据集映射到高维特征空间。

## 五、kd树

### 1. 问题导入

实现k近邻法时，主要考虑的问题是如何对训练数据进行快速k近邻搜索。这在特征空间的维数大及训练数据容量大时尤其必要。

**k近邻法最简单的实现是线性扫描（穷举搜索），即要计算输入实例与每一个训练实例的距离。计算并存储好以后，再查找K近邻。**当训练集很大时，计算非常耗时。

为了提高kNN搜索的效率，可以考虑使用特殊的结构存储训练数据，以减小计算距离的次数。

### 2. kd树简介

根据**KNN**每次需要预测一个点时，我们都需要计算训练数据集里每个点到这个点的距离，然后选出距离最近的k个点进行投票。**当数据集很大时，这个计算成本非常高，针对N个样本，D个特征的数据集，其算法复杂度为O（DN²）**。

**kd树**：为了避免每次都重新计算一遍距离，算法会把距离信息保存在一棵树里，这样在计算之前从树里查询距离信息，尽量避免重新计算。其基本原理是，**如果A和B距离很远，B和C距离很近，那么A和C的距离也很远**。有了这个信息，就可以在合适的时候跳过距离远的点。

这样优化后的算法复杂度可降低到**O（DNlog（N））**。感兴趣的读者可参阅论文：Bentley，J.L.，Communications of the ACM（1975）。

1989年，另外一种称为**Ball Tree**的算法，在kd Tree的基础上对性能进一步进行了优化。感兴趣的读者可以搜索**Five balltree construction algorithms**来了解详细的算法信息。

### 3. kd树原理

![image-20190213191654082](https://raw.githubusercontent.com/sanmaomashi/Salute_Machine_Learning/main/img/5.png)

黄色的点作为根节点，上面的点归左子树，下面的点归右子树，接下来再不断地划分，分割的那条线叫做分割超平面（splitting hyperplane），在一维中是一个点，二维中是线，三维的是面。

![image-20190213191739222](https://raw.githubusercontent.com/sanmaomashi/Salute_Machine_Learning/main/img/6.png)

黄色节点就是Root节点，下一层是红色，再下一层是绿色，再下一层是蓝色。

![image-20190219101722826](https://raw.githubusercontent.com/sanmaomashi/Salute_Machine_Learning/main/img/7.png)

**1.树的建立；**

**2.最近邻域搜索（Nearest-Neighbor Lookup）**

kd树(K-dimension tree)是**一种对k维空间中的实例点进行存储以便对其进行快速检索的树形数据结构。**kd树是一种二叉树，表示对k维空间的一个划分，**构造kd树相当于不断地用垂直于坐标轴的超平面将K维空间切分，构成一系列的K维超矩形区域**。kd树的每个结点对应于一个k维超矩形区域。**利用kd树可以省去对大部分数据点的搜索，从而减少搜索的计算量。**

![image-20190213223817957](https://raw.githubusercontent.com/sanmaomashi/Salute_Machine_Learning/main/img/8.png)

类比“二分查找”：给出一组数据：[9 1 4 7 2 5 0 3 8]，要查找8。如果挨个查找（线性扫描），那么将会把数据集都遍历一遍。而如果排一下序那数据集就变成了：[0 1 2 3 4 5 6 7 8 9]，按前一种方式我们进行了很多没有必要的查找，现在如果我们以5为分界点，那么数据集就被划分为了左右两个“簇” [0 1 2 3 4]和[6 7 8 9]。

因此，根本就没有必要进入第一个簇，可以直接进入第二个簇进行查找。把二分查找中的数据点换成k维数据点，这样的划分就变成了用超平面对k维空间的划分。空间划分就是对数据点进行分类，“挨得近”的数据点就在一个空间里面。

### 4. kd树构造方法

1）构造根结点，使根结点对应于K维空间中包含所有实例点的超矩形区域；

（2）通过递归的方法，不断地对k维空间进行切分，生成子结点。在超矩形区域上选择一个坐标轴和在此坐标轴上的一个切分点，确定一个超平面，这个超平面通过选定的切分点并垂直于选定的坐标轴，将当前超矩形区域切分为左右两个子区域（子结点）；这时，实例被分到两个子区域。

（3）上述过程直到子区域内没有实例时终止（终止时的结点为叶结点）。在此过程中，将实例保存在相应的结点上。

（4）通常，循环的选择坐标轴对空间切分，选择训练实例点在坐标轴上的中位数为切分点，这样得到的kd树是平衡的（平衡二叉树：它是一棵空树，或其左子树和右子树的深度之差的绝对值不超过1，且它的左子树和右子树都是平衡二叉树）。

KD树中每个节点是一个向量，和二叉树按照数的大小划分不同的是，KD树每层需要选定向量中的某一维，然后根据这一维按左小右大的方式划分数据。在构建KD树时，关键需要解决2个问题：

**（1）选择向量的哪一维进行划分；**

**（2）如何划分数据；**

第一个问题简单的解决方法可以是随机选择某一维或按顺序选择，但是**更好的方法应该是在数据比较分散的那一维进行划分（分散的程度可以根据方差来衡量）**。好的划分方法可以使构建的树比较平衡，可以每次选择中位数来进行划分，这样问题2也得到了解决。

### 5. kd树案例分析

#### 步骤一：树的建立

![image-20190219102142984](https://raw.githubusercontent.com/sanmaomashi/Salute_Machine_Learning/main/img/9.png)

（1）思路引导：

根结点对应包含数据集T的矩形，选择x(1)轴，6个数据点的x(1)坐标中位数是6，这里选最接近的(7,2)点，以平面x(1)=7将空间分为左、右两个子矩形（子结点）；接着左矩形以x(2)=4分为两个子矩形（左矩形中{(2,3),(5,4),(4,7)}点的x(2)坐标中位数正好为4），右矩形以x(2)=6分为两个子矩形，如此递归，最后得到如下图所示的特征空间划分和kd树。

![image-20190219102409567](https://raw.githubusercontent.com/sanmaomashi/Salute_Machine_Learning/main/img/10.png)

#### 步骤2：最近领域的搜索

假设标记为星星的点是 test point， 绿色的点是找到的近似点，在回溯过程中，需要用到一个队列，存储需要回溯的点，在判断其他子节点空间中是否有可能有距离查询点更近的数据点时，做法是以查询点为圆心，以当前的最近距离为半径画圆，这个圆称为候选超球（candidate hypersphere），如果圆与回溯点的轴相交，则需要将轴另一边的节点都放到回溯队列里面来。

![image-20190213224152601](https://raw.githubusercontent.com/sanmaomashi/Salute_Machine_Learning/main/img/11.png)

样本集{(2,3),(5,4), (9,6), (4,7), (8,1), (7,2)}

#### 步骤3:查找点(2.1,3.1)

![image-20190213224414342](https://raw.githubusercontent.com/sanmaomashi/Salute_Machine_Learning/main/img/12.png)

在(7,2)点测试到达(5,4)，在(5,4)点测试到达(2,3)，然后search_path中的结点为<(7,2),(5,4), (2,3)>，从search_path中取出(2,3)作为当前最佳结点nearest, dist为0.141；

然后回溯至(5,4)，以(2.1,3.1)为圆心，以dist=0.141为半径画一个圆，并不和超平面y=4相交，如上图，所以不必跳到结点(5,4)的右子空间去搜索，因为右子空间中不可能有更近样本点了。

于是再回溯至(7,2)，同理，以(2.1,3.1)为圆心，以dist=0.141为半径画一个圆并不和超平面x=7相交，所以也不用跳到结点(7,2)的右子空间去搜索。

至此，search_path为空，结束整个搜索，返回nearest(2,3)作为(2.1,3.1)的最近邻点，最近距离为0.141。

#### 步骤4:查找点(2,4.5)

![image-20190219103050940](https://raw.githubusercontent.com/sanmaomashi/Salute_Machine_Learning/main/img/13.png)

在(7,2)处测试到达(5,4)，在(5,4)处测试到达(4,7)【优先选择在本域搜索】，然后search_path中的结点为<(7,2),(5,4), (4,7)>，从search_path中取出(4,7)作为当前最佳结点nearest, dist为3.202；

然后回溯至(5,4)，以(2,4.5)为圆心，以dist=3.202为半径画一个圆与超平面y=4相交，所以需要跳到(5,4)的左子空间去搜索。所以要将(2,3)加入到search_path中，现在search_path中的结点为<(7,2),(2, 3)>；另外，(5,4)与(2,4.5)的距离为3.04 < dist = 3.202，所以将(5,4)赋给nearest，并且dist=3.04。

回溯至(2,3)，(2,3)是叶子节点，直接平判断(2,3)是否离(2,4.5)更近，计算得到距离为1.5，所以nearest更新为(2,3)，dist更新为(1.5)

回溯至(7,2)，同理，以(2,4.5)为圆心，以dist=1.5为半径画一个圆并不和超平面x=7相交, 所以不用跳到结点(7,2)的右子空间去搜索。

至此，search_path为空，结束整个搜索，返回nearest(2,3)作为(2,4.5)的最近邻点，最近距离为1.5。

#### 总结

首先**通过二叉树搜索**（比较待查询节点和分裂节点的分裂维的值，小于等于就进入左子树分支，大于就进入右子树分支直到叶子结点），**顺着“搜索路径”很快能找到最近邻的近似点**，也就是与待查询点处于同一个子空间的叶子结点；

然后再回溯搜索路径，并判断搜索路径上的结点的其他子结点空间中是否可能有距离查询点更近的数据点，如果有可能，则需要跳到其他子结点空间中去搜索（将其他子结点加入到搜索路径）。

重复这个过程直到搜索路径为空。

## 六、案例分析

本案例使用最著名的”鸢尾“数据集，该数据集曾经被Fisher用在经典论文中，目前作为教科书般的数据样本预存在Scikit-learn的工具包中。

![img](https://raw.githubusercontent.com/sanmaomashi/Salute_Machine_Learning/main/img/14.png)

### 1. 读入Iris数据集细节资料

```python
from sklearn.datasets import load_iris
# 使用加载器读取数据并且存入变量iris
iris = load_iris()

iris.data.shape # 查验数据规模
X = iris.data  # 特征数据
y = iris.target  # 类别标签

# 查看数据说明（这是一个好习惯）
print("鸢尾花数据集的返回值：\n", iris)
# 返回值是一个继承自字典的Bench
print("鸢尾花的特征值:\n", iris["data"])
print("鸢尾花的目标值：\n", iris.target)
print("鸢尾花特征的名字：\n", iris.feature_names)
print("鸢尾花目标值的名字：\n", iris.target_names)
print("鸢尾花的描述：\n", iris.DESCR)
```

通过上述代码对数据的查验以及数据本身的描述，我们了解到Iris数据集共有150朵鸢尾数据样本，并且均匀分布在3个不同的亚种；每个数据样本有总共4个不同的关于花瓣、花萼的形状特征所描述。由于没有制定的测试集合，因此按照惯例，我们需要对数据进行随即分割，25%的样本用于测试，其余75%的样本用于模型的训练。

由于不清楚数据集的排列是否随机，可能会有按照类别去进行依次排列，这样训练样本的不均衡的，所以我们需要分割数据，已经默认有随机采样的功能。

### 2. 对Iris数据集进行分割

```python
from sklearn.cross_validation import train_test_split

# 将数据集划分为训练集和测试集
X_train,X_test,y_train,y_test = train_test_split(iris.data,iris.target,test_size=0.25,random_state=42)
```

### 3. 对特征数据进行标准化

```python
from sklearn.preprocessing import StandardScaler

# 特征缩放（标准化）
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
```

### 4. 模型训练

```python
k = 3  # 设置K值
knn = KNeighborsClassifier(n_neighbors=k)
knn.fit(X_train, y_train)
```

### 5. 预测并评估模型

```python
y_pred = knn.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print("准确率：", accuracy)
```

在这个示例中，我们使用load_iris()函数导入"鸢尾"数据集，然后将数据集划分为训练集和测试集。接下来，我们使用StandardScaler对特征进行标准化处理。然后，我们使用KNeighborsClassifier类创建KNN分类器，并指定K值为3。使用fit()方法对模型进行训练。最后，使用predict()方法对测试集进行预测，并使用accuracy_score()函数计算准确率。

请注意，上述示例是一个简化的KNN算法演示过程。在实际应用中，可能还需要进行交叉验证、调优K值等步骤来优化模型性能。

## 七、KNN算法优缺点

KNN算法（K-Nearest Neighbors）是一种简单但强大的分类和回归算法。它有以下的优点和缺点：

优点：

1. 简单直观：KNN算法的原理简单，易于理解和实现。
2. 无需训练过程：KNN算法不需要显式的训练过程，只需保存训练样本即可进行预测。
3. 适用性广泛：KNN算法适用于分类和回归问题，并且可以处理多分类问题。
4. 对异常值鲁棒性强：KNN算法对于异常值和噪声的影响相对较小，因为它考虑了多个最近邻居的投票结果。

缺点：

1. 计算复杂度高：KNN算法需要计算测试样本与所有训练样本之间的距离，当训练集较大时，计算复杂度较高。
2. 内存消耗大：KNN算法需要保存所有的训练样本，对于大规模数据集，需要较大的内存空间。
3. 数据不平衡问题：当训练样本中某个类别的数量明显多于其他类别时，KNN算法可能会偏向于多数类别。
4. 需要选择合适的K值：KNN算法的性能受到K值的影响，选择不合适的K值可能导致欠拟合或过拟合。

为了克服KNN算法的缺点，可以考虑使用加权KNN算法、使用特征选择减少特征维度、使用距离加权等技术进行改进和优化。

## 八、总结

KNN算法具有简单直观、适用性广泛和对异常值鲁棒性强的优点，但也存在计算复杂度高、内存消耗大、数据不平衡问题和需要选择合适的K值等缺点。在实际应用中，需要根据具体问题和数据集的特点来权衡和选择使用KNN算法。

